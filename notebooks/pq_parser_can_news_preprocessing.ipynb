{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oAAlt4oviFp"
      },
      "source": [
        "## Parse ProQuest Metadata\n",
        "This notebook includes a python function to parse newspaper articles downloaded from ProQuest Global Newsstream into one CSV file with metadata and full text (when full text is available).\n",
        "\n",
        "#### Download PQ files to use as input\n",
        "The script below takes as input .txt file downloads available via ProQuest Global Newsstream. These are available from ProQuest in batches of up to 100 articles per file. To save those files from your ProQuest search results:\n",
        "1. Select each article you want to save (or select all results on page) using result checkboxes.\n",
        "2. From the *...* button dropdown, select \"TXT - Text Only\"\n",
        "\n",
        "    ![Screenshot of Saving results from Proquest](imgs/pq_save_feb20.png \"Save results from PQ\")\n",
        "3. Accept all defaults and continue to save .txt file of bundled article downloads.\n",
        "4. Save the downloaded .txt file (or files) to a folder in the same directory as this notebook. In the example below, that folder is called \"txt_input\" but you can use any path name that you will then call in the final cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XWVl5ydDviF5"
      },
      "outputs": [],
      "source": [
        "## Step 1: import libraries required to run Python code\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import csv\n",
        "import glob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzycGjPEviF-"
      },
      "source": [
        "### Collect metadata\n",
        "We need to tell the script which fields to collect from the .txt files. You can inspect the text files yourself to look for field names at the beginning of new lines such as `Title: ` or `Publication year: ` and then add them to the list variable called `fieldnames` below. Here is a list of field names available in the ProQuest text downloads as of July 2019:\n",
        "\n",
        "`'Title', 'Publication title', 'Publication year', 'Document URL', 'Full text', 'Links', 'Section', 'Publication subject', 'ISSN', 'Copyright', 'Abstract', 'Publication info', 'Last updated', 'Place of publication', 'Location', 'Author', 'Publisher', 'Identifier / keyword', 'Source type', 'ProQuest document ID', 'Country of publication', 'Language of publication', 'Publication date', 'Subject', 'Database', 'Document type'`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BhHZBajpviGA"
      },
      "outputs": [],
      "source": [
        "fieldnames = ['Titre','Année de publication','Texte intégral','Publication', 'Lieu de publication', 'Auteur', 'Éditeur', 'Date de publication', 'Sujet', 'Société / organisation']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdLeOVW2viGC"
      },
      "source": [
        "### Running the script\n",
        "The function below:\n",
        "1. Accepts a path to a directory full of .txt files you want to process as its argument (e.g., `parsePQ(\"txt_input/\")`.\n",
        "2. Creates a csv file called `pq_metadata.csv`\n",
        "3. Cycles through every text file in the path you identified in step 1.\n",
        "4. Splits each document into individual articles using the `sep` separator.\n",
        "5. Splits each article into lines.\n",
        "6. For each line, matches `fieldnames` with existing metadata tags in each document.\n",
        "7. Saves the fieldname and following content (values) to a dictionary, `metadata_dict`.\n",
        "7. Writes the fieldnames (keys) and content (values) of the `metadata_dict` for each article to its own row in the CSV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Z-NdRq3IviGD"
      },
      "outputs": [],
      "source": [
        "sep = \"____________________________________________________________\"\n",
        "\n",
        "## function that takes a directory of .txt files from ProQuest as input\n",
        "def parsePQ(path, output_path, file_output='csv'):\n",
        "    '''This function parses text file downloads from ProQuest into metadata and full-text.\n",
        "\n",
        "    It takes as input a path to .txt files that have been downloaded from ProQuest Global Newsstream.\n",
        "    It returns a CSV file of selected metadata along with the full text for each article as a separate row.\n",
        "    Optional: set the second parameter to 'txt' to also output individual articles as .txt files in /output/\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    path : str\n",
        "        path to .txt files that will be parsed (e.g., 'txts/' or 'pdfs/')\n",
        "    file_output : str (default = 'csv')\n",
        "        change to file_output='txt' to return individual articles as text files AND a CSV file for all;\n",
        "        default behavior only outputs the CSV file\n",
        "    '''\n",
        "    # open a csv file to write metadata to\n",
        "    with open(output_path, 'w', newline='', encoding='utf-8-sig') as csvfile:\n",
        "        # add fieldnames as header\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        #cycle through every text file in the directory given as an argument\n",
        "        files_all = [os.path.join(path, f) for f in os.listdir(path) if f.endswith(\".txt\")]\n",
        "        \n",
        "        print(list(files_all))\n",
        "\n",
        "        for filename in files_all:\n",
        "\n",
        "            #remove the path, whitespace, and '.txt' from filename to later use when printing output\n",
        "            file_id = filename[:-4].strip(path)\n",
        "\n",
        "            with open(filename, 'r', encoding='utf-8-sig') as in_file:\n",
        "                # text var for string of all docs\n",
        "                text = in_file.read()\n",
        "\n",
        "                # split string by separator into single articles\n",
        "                docs = re.split(sep, text)\n",
        "\n",
        "                # remove first and last items from docs list: first item is empty string; last is copyright info\n",
        "                docs = docs[1:-1]\n",
        "\n",
        "                # loop through every doc to collect metadata and full text\n",
        "                for i, doc in enumerate(docs):\n",
        "                    if file_output == 'txt':\n",
        "                        new_file = 'output/' + file_id + str(i) + '.txt'\n",
        "                        txt_file = open(new_file,'w')\n",
        "                    # remove white space from beginning and end of each article\n",
        "                    doc = doc.strip()\n",
        "\n",
        "                    # skip any empty docs\n",
        "                    if doc==\"\":\n",
        "                        continue\n",
        "\n",
        "                    if file_output == 'txt':\n",
        "                        txt_file.write(doc)\n",
        "                        txt_file.close()\n",
        "\n",
        "                    # split doc on every new line\n",
        "                    metadata_lines = doc.split('\\n\\n')\n",
        "\n",
        "                    #remove first \"line\" from article which is the article title without any field title\n",
        "                    metadata_lines = metadata_lines[1:]\n",
        "\n",
        "                    #declare a new dictionary\n",
        "                    metadata_dict = {}\n",
        "\n",
        "                    #for each element add the fieldname/key and following value to a dictionary\n",
        "                    for line in metadata_lines:\n",
        "\n",
        "                        #ignore lines that do not have a field beginning \"Xxxxxx:\" (e.g. \"Publication title: \")\n",
        "                        if not re.match(r'^[^:]+: ', line):\n",
        "                            continue\n",
        "                        #looks for beginning of new line following structure of \"Publication year: \" splitting on the colon space\n",
        "                        (key,value) = line.split(sep=': ', maxsplit=1)\n",
        "\n",
        "                        #only add to dictionary if the key is in fieldnames\n",
        "                        if key in fieldnames:\n",
        "                            metadata_dict[key] = value\n",
        "\n",
        "                    #write the dictionary values to new row in csv\n",
        "                    writer.writerow(metadata_dict)\n",
        "            print(\"Writing\", file_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfRznF7nviGG"
      },
      "source": [
        "### Running the script\n",
        "Running the cell above loads the function, but doesn't do anything yet.\n",
        "To *execute* the function, run the cell below, replacing `txt_input/` with the name of your folder full of .txt files.\n",
        "\n",
        "Add a second parameter, file_output='txt', if you would like to return individual articles as text files as well as the CSV of the full parsed data. In this case make sure there is an ```/output/``` directory available to store the text files.\n",
        "\n",
        "```parsePQ(\"txt_input/\", file_output='txt')```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "parti = \"GPC\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "rZg1dg50viGI",
        "outputId": "d5b127f4-0545-4a01-d84e-2c77aec1f29b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['/home/beetho/Downloads/test_scrap_results/Canada Policy/GPC_old/2.txt', '/home/beetho/Downloads/test_scrap_results/Canada Policy/GPC_old/1.txt']\n",
            "Writing 2\n",
            "Writing 1\n"
          ]
        }
      ],
      "source": [
        "#run the script\n",
        "parsePQ(f\"/home/beetho/Downloads/test_scrap_results/Canada Policy/{parti}_old\", f\"/home/beetho/Downloads/test_scrap_results/{parti}.csv\")   # C:/Users/Perron/Desktop/Chris/Download/Canada/ # \"C:\\Users\\firmi\\Downloads\\All_proquest_data\\USnews_update\\articles\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wxfaaqOviGM"
      },
      "source": [
        "### Et voila!\n",
        "You should now see pq_metadata.csv in the same directory as this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "bq = pd.read_csv(f\"/home/beetho/Downloads/test_scrap_results/BQ.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "41"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(bq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "pcc = pd.read_csv(f\"/home/beetho/Downloads/test_scrap_results/PCC.csv\")\n",
        "ndp = pd.read_csv(f\"/home/beetho/Downloads/test_scrap_results/NDP.csv\")\n",
        "lpc = pd.read_csv(f\"/home/beetho/Downloads/test_scrap_results/LPC.csv\")\n",
        "gpc = pd.read_csv(f\"/home/beetho/Downloads/test_scrap_results/GPC.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "bq[\"Parti\"] = \"BQ\"\n",
        "pcc[\"Parti\"] = \"PCC\"\n",
        "lpc[\"Parti\"] = \"LPC\"\n",
        "ndp[\"Parti\"] = \"NDP\"\n",
        "gpc[\"Parti\"] = \"GPC\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_parties = pd.concat([bq, pcc, lpc, ndp, gpc], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_parties.drop_duplicates(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['Titre', 'Année de publication', 'Texte intégral', 'Publication',\n",
              "       'Lieu de publication', 'Auteur', 'Éditeur', 'Date de publication',\n",
              "       'Sujet', 'Société / organisation', 'Parti'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_parties.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_parties.drop_duplicates(subset=[\"Titre\",\"Texte intégral\",\"Date de publication\",\"Parti\"], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reset index after dropping duplicates\n",
        "all_parties.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4462"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(all_parties)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_parties.to_csv(f\"/home/beetho/Downloads/test_scrap_results/all_parties.csv\", index=False, encoding='utf-8-sig')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
